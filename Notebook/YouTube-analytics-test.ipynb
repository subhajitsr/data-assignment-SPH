{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75108fd1",
   "metadata": {},
   "source": [
    "## Define Youtube Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40787b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from typing import Any, Optional, Dict, List\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "class CredentialError(Exception):\n",
    "    \"\"\"This exception is raised when there is error in creating credential.\"\"\"\n",
    "\n",
    "class YoutubeDataError(Exception):\n",
    "    \"\"\"This exception is raised when there is error in creating Youtube data object.\"\"\"\n",
    "\n",
    "class InsufficientInputError(Exception):\n",
    "    \"\"\"This exception is raised when insufficint info is provided to create channel object.\"\"\"\n",
    "\n",
    "class ChannelNotFoundError(Exception):\n",
    "    \"\"\"This exception is raised when a channel is not found.\"\"\"\n",
    "\n",
    "class YoutubeChannel():\n",
    "    def __init__(self,\n",
    "                 service_account_info: json,\n",
    "                 scopes: Optional[list] = ['https://www.googleapis.com/auth/youtube.readonly'],\n",
    "                 channel_name: Optional[str] = None,\n",
    "                channel_id: Optional[str] = None) -> None:\n",
    "        # Trying to create the credential object\n",
    "        try:\n",
    "            credentials = service_account.Credentials.from_service_account_info(\n",
    "            service_account_info,\n",
    "            scopes=scopes\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CredentialError(e)\n",
    "        \n",
    "        # Trying to build the youtube object\n",
    "        try:\n",
    "            youtube = build('youtube', 'v3', credentials=credentials)\n",
    "        except Exception as e:\n",
    "            raise YoutubeDataError(e)\n",
    "            \n",
    "        # Fetching Channel id by name\n",
    "        if channel_name is None and channel_id is None:\n",
    "            raise InsufficientInputError(\"Either channel_name or channel_id needs to be provided\")\n",
    "        \n",
    "        if channel_id is None:\n",
    "            # Fetch channel ID by name\n",
    "            response = youtube.search().list(q=channel_name, type='channel', part='id', maxResults=1).execute()\n",
    "            if not response.get('items'):\n",
    "                raise ChannelNotFoundError(f\"No channel found for username: {channel_name}\")\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "        else:\n",
    "            # Verify the channel id passed is correct\n",
    "            if not youtube.channels().list(id=channel_id,part='id').execute().get('items'):\n",
    "                raise ChannelNotFoundError(f\"No channel found for channel_id: {channel_id}\")\n",
    "        \n",
    "        # Get channel attributes\n",
    "        response = youtube.channels().list(\n",
    "            id=channel_id,\n",
    "            part='snippet,statistics'\n",
    "        ).execute()\n",
    "        \n",
    "        # Set Channel attribute for the object\n",
    "        self._credentials = credentials\n",
    "        self._youtube = youtube\n",
    "        self.channel_name = channel_name\n",
    "        self.channel_id = channel_id\n",
    "        self.title = response.get('items')[0].get('snippet').get('title')\n",
    "        self.description = response.get('items')[0].get('snippet').get('description')\n",
    "        self.customUrl = response.get('items')[0].get('snippet').get('customUrl')\n",
    "        self.publishedAt = response.get('items')[0].get('snippet').get('publishedAt')\n",
    "        self.country = response.get('items')[0].get('snippet').get('country')\n",
    "        self.viewCount = response.get('items')[0].get('statistics').get('viewCount')\n",
    "        self.subscriberCount = response.get('items')[0].get('statistics').get('subscriberCount')\n",
    "        self.videoCount = response.get('items')[0].get('statistics').get('videoCount')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_video_statistics(youtube: Any, video_ids: list) -> list:\n",
    "        video_stats = []\n",
    "\n",
    "        # Fetch statistics for the videos\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=','.join(video_ids)\n",
    "        ).execute()\n",
    "\n",
    "        for item in video_response['items']:\n",
    "            video_stats.append({\n",
    "                'id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                'views': item['statistics'].get('viewCount',0),\n",
    "                'likes': item['statistics'].get('likeCount',0),\n",
    "                'dislikes': item['statistics'].get('dislikeCount',0),\n",
    "                'comments': item['statistics'].get('commentCount', 0),\n",
    "                'publishedAt': item['snippet']['publishedAt']\n",
    "            })\n",
    "    \n",
    "        return video_stats\n",
    "\n",
    "    \n",
    "    def get_video_data(self, chunk_size: Optional[int] = 50, days_count: Optional[int] = 365):\n",
    "        video_data = []\n",
    "        \n",
    "        # Calculating published_after based on days_count\n",
    "        t_ago = datetime.datetime.now() - datetime.timedelta(days=days_count)\n",
    "        published_after = t_ago.isoformat(\"T\") + \"Z\"\n",
    "\n",
    "        request = self._youtube.search().list(\n",
    "                    part='id',\n",
    "                    channelId=self.channel_id,\n",
    "                    publishedAfter=published_after,\n",
    "                    maxResults=chunk_size,\n",
    "                    type='video'\n",
    "                )\n",
    "\n",
    "        while request:\n",
    "            response = request.execute()\n",
    "            video_ids = [item['id']['videoId'] for item in response['items']]\n",
    "            \n",
    "            # Getting video statistics\n",
    "            video_data = video_data + self.get_video_statistics(self._youtube, video_ids)\n",
    "            \n",
    "            # Creating request for the next chunk fetch\n",
    "            request = self._youtube.search().list_next(request, response)        \n",
    "                \n",
    "        return video_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af31885",
   "metadata": {},
   "source": [
    "## Define Snowflake Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd831cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from typing import Any, Optional, Dict, List\n",
    "import logging\n",
    "        \n",
    "        \n",
    "class SnowflakeLoader():\n",
    "    def __init__(self,\n",
    "                 conn: Any,\n",
    "                 schema: str,\n",
    "                 s3_stage_name: str,\n",
    "                 stage_table_name: str,\n",
    "                 core_table_name: str,\n",
    "                 s3_col_map: Dict,\n",
    "                 load_type: Optional[str] = 'FULL',\n",
    "                 merge_on_col: Optional[List[str]] = []\n",
    "                ):\n",
    "        try:\n",
    "            curs = conn.cursor()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error while opening the cursor. {e}\")\n",
    "        \n",
    "        # Check when load type is Merge, the merge_on_col need to be supplied\n",
    "        if load_type.upper() == 'MERGE' and len(merge_on_col) == 0:\n",
    "            raise Exception(f\"merge_on_col arg is mandatory for Loader type: {load_type}\")\n",
    "        \n",
    "        # Fecthing stage table columns\n",
    "        curs.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{stage_table_name.upper()}' AND TABLE_SCHEMA = '{schema.upper()}';\n",
    "        \"\"\")\n",
    "        self.stg_cols = [row[0] for row in curs.fetchall()]\n",
    "        \n",
    "        # Fecthing main table columns\n",
    "        curs.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{core_table_name.upper()}' AND TABLE_SCHEMA = '{schema.upper()}';\n",
    "        \"\"\")\n",
    "        self.main_cols = [row[0] for row in curs.fetchall()]\n",
    "        \n",
    "        # Checking if stage and main table columns are in sync\n",
    "        if not set(self.main_cols) == set(self.stg_cols):\n",
    "            raise Exception('Stage and Main table column mismatch. ')\n",
    "        \n",
    "        # Converting merge columns to upper\n",
    "        if  len(merge_on_col) != 0:\n",
    "            self.merge_on_col = [col.upper() for col in merge_on_col]\n",
    "        else:\n",
    "            self.merge_on_col = merge_on_col\n",
    "            \n",
    "        # Checking if merge col existing in stage and main table\n",
    "        if load_type.upper() == 'MERGE':\n",
    "            if len([col for col in self.merge_on_col if col in self.main_cols and col in self.stg_cols]) != len(self.merge_on_col):\n",
    "                raise Exception('All columns in merge_on_col must be present in both stage and core table.')\n",
    "        \n",
    "            \n",
    "        self.schema = schema.upper()\n",
    "        self.stage_table_name = stage_table_name.upper()\n",
    "        self.core_table_name = core_table_name.upper()\n",
    "        self.load_type = load_type.upper()\n",
    "        self.conn = conn\n",
    "        self.s3_col_map = s3_col_map\n",
    "        self.s3_stage_name = s3_stage_name\n",
    "        \n",
    "        curs.close()\n",
    "    \n",
    "    def curs_handler(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            try:\n",
    "                curs = self.conn.cursor()\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"from db_conn_check: Error while opening the cursor. {e}\")\n",
    "            func(self, curs, *args, **kwargs)\n",
    "            curs.close()\n",
    "        return wrapper\n",
    "\n",
    "    \n",
    "    @curs_handler\n",
    "    def stg_to_core(self, curs, *args, **kwargs) -> None:\n",
    "            \n",
    "        if self.load_type == 'MERGE':\n",
    "            sql_text = [f\"\"\"\n",
    "                MERGE INTO {self.schema}.{self.core_table_name} as t\n",
    "                USING {self.schema}.{self.stage_table_name} as d\n",
    "                ON \n",
    "                {'AND '.join(f\"d.{col} = t.{col} \" for col in self.merge_on_col)}\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET\n",
    "                {', '.join(f\"{col} = d.{col} \" for col in [col for col in self.main_cols if col not in self.merge_on_col])}\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT\n",
    "                ({','.join([col for col in self.main_cols])})\n",
    "                VALUES\n",
    "                ({','.join([f\"d.{col}\" for col in self.main_cols])})\n",
    "                ;\n",
    "                \"\"\"]\n",
    "        else:\n",
    "            sql_text = [f\"delete from {self.schema}.{self.core_table_name};\",\n",
    "            f\"\"\"insert into {self.schema}.{self.core_table_name}\n",
    "            ({','.join([col for col in self.main_cols])})\n",
    "            select\n",
    "            {','.join([col for col in self.main_cols])}\n",
    "            from\n",
    "            {self.schema}.{self.stage_table_name};\n",
    "            \"\"\"]\n",
    "        \n",
    "        for sql in sql_text:\n",
    "            curs.execute(sql)\n",
    "            logging.info(sql)\n",
    "\n",
    "    @curs_handler  \n",
    "    def s3_to_stg(self, curs, *args, **kwargs) -> None:\n",
    "        \n",
    "        # Validate S3_col_map dictionary if available\n",
    "        for item in self.s3_col_map.items():\n",
    "            if item[1].upper() not in self.stg_cols:\n",
    "                raise Exception(f\"s3_to_stg: Column {item[1]} not present in the table {self.schema}.{self.stage_table_name}\")\n",
    "                    \n",
    "        \n",
    "        # Cleanup stage table first\n",
    "        curs.execute(f\"\"\"delete from {self.schema}.{self.stage_table_name};\"\"\")\n",
    "        \n",
    "        # Prepare the COPY INTO statement for S3 load\n",
    "        sql_text = f\"\"\"\n",
    "        COPY INTO {self.schema}.{self.stage_table_name} ({','.join([col for col in [item[1] for item in self.s3_col_map.items()]])})\n",
    "        FROM (\n",
    "            SELECT\n",
    "            {','.join([f\"$1:{item[0]}::VARIANT AS {item[1]}\" for item in self.s3_col_map.items()])}\n",
    "            FROM @{self.s3_stage_name}\n",
    "        )\n",
    "        FILE_FORMAT = (TYPE = 'PARQUET');\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(sql_text)\n",
    "        curs.execute(sql_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d89bb",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_info = json.load(open('Secrets/youtube-analytics-sph-3bbd40b130f0.json'))\n",
    "channel_list = ['straitstimesonline', 'BeritaHarianSG1957', 'Tamil_Murasu', 'TheBusinessTimes', 'zaobaodotsg']\n",
    "temp_directory = './temp_download'\n",
    "s3_bucket_name = 'youtube-stats-001'\n",
    "s3_path = \"dump/parquet\"\n",
    "aws_access_key_id = \"********\"\n",
    "aws_secret_access_key = \"************\"\n",
    "sf_username = '**********'\n",
    "sf_password = '***********'\n",
    "sf_account = '***********'\n",
    "sf_warehouse = 'COMPUTE_WH'\n",
    "sf_database = 'TESTDB'\n",
    "sf_schema = 'CORE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf8501",
   "metadata": {},
   "source": [
    "## Fetch from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f12fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "df_channel = pd.DataFrame(columns=['channel_name','channel_id','title','customUrl','publishedAt','country','viewCount','subscriberCount','videoCount','etl_ts'])\n",
    "df_video = pd.DataFrame(columns=['id','title','url','views','likes','dislikes','comments','publishedAt','channel_name','channel_id','rptg_dt','etl_ts'])\n",
    "\n",
    "_now_ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "_today_dt = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "logging.info(f\"_now_ts: {_now_ts}\")\n",
    "logging.info(f\"_today_dt: {_today_dt}\")\n",
    "\n",
    "for channel_name in channel_list:\n",
    "    logging.info(f\"Fetching data for Channel: {channel_name}\")\n",
    "    channelObj = YoutubeChannel(service_account_info=service_account_info, channel_name=channel_name)    \n",
    "    df_channel = pd.concat([df_channel, pd.DataFrame([{\n",
    "        'channel_name': channelObj.channel_name,\n",
    "        'channel_id': channelObj.channel_id,\n",
    "        'title': channelObj.title,\n",
    "        'customUrl': channelObj.customUrl,\n",
    "        'publishedAt': channelObj.publishedAt,\n",
    "        'country': channelObj.country,\n",
    "        'viewCount': channelObj.viewCount,\n",
    "        'subscriberCount': channelObj.subscriberCount,\n",
    "        'videoCount': channelObj.videoCount,\n",
    "        'etl_ts': _now_ts\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    \n",
    "    df_video_temp = pd.DataFrame(channelObj.get_video_data())\n",
    "    df_video_temp['channel_name'] = channelObj.channel_name\n",
    "    df_video_temp['channel_id'] = channelObj.channel_id\n",
    "    df_video_temp['rptg_dt'] = _today_dt\n",
    "    df_video_temp['etl_ts'] = _now_ts\n",
    "    df_video = pd.concat([df_video, df_video_temp], ignore_index=True)\n",
    "\n",
    "df_video_md = df_video[['id','channel_id','title','url','publishedAt','etl_ts']]\n",
    "df_video = df_video[['id','channel_id','rptg_dt','views','likes','dislikes','comments','etl_ts']]\n",
    "\n",
    "# Dropping duplicates based on respective Key columns\n",
    "df_channel = df_channel.drop_duplicates(subset=['channel_id'])\n",
    "df_video_md = df_video_md.drop_duplicates(subset=['id'])\n",
    "df_video = df_video.drop_duplicates(subset=['id','rptg_dt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0045517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_channel)\n",
    "display(df_video_md)\n",
    "display(df_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afa087",
   "metadata": {},
   "source": [
    "## Write to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4637648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "# Upload the Channel Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_channel.to_parquet(parquet_buffer, index=False)\n",
    "    chnl_file_name = f\"channel_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/channel/{chnl_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {chnl_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/channel\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Channel MD S3 Upload failed. {e}\")\n",
    "\n",
    "# Upload the Video MD Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_video_md.to_parquet(parquet_buffer, index=False)\n",
    "    video_md_file_name = f\"video_md_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/video_md/{video_md_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {video_md_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/video_md\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Video MD S3 Upload failed. {e}\")\n",
    "\n",
    "# Upload the Video Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_video.to_parquet(parquet_buffer, index=False)\n",
    "    video_file_name = f\"video_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/video/{video_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {video_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/video\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Video Stats S3 Upload failed. {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c619351",
   "metadata": {},
   "source": [
    "## Load to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e653a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "dbcon = snowflake.connector.connect(\n",
    "    user=sf_username,\n",
    "    password=sf_password,\n",
    "    account=sf_account,\n",
    "    warehouse=sf_warehouse,\n",
    "    database=sf_database,\n",
    "    schema=sf_schema\n",
    ")\n",
    "\n",
    "logging.info(\"Start: Loading Channel MD.\")\n",
    "# Load Channel MD\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_channel_md',\n",
    "     stage_table_name = 'tbl_stg_yt_channel_md',\n",
    "     core_table_name = 'tbl_yt_channel_md',\n",
    "     s3_col_map = {\n",
    "                    'channel_name': 'channel_name',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'title': 'title',\n",
    "                    'customUrl': 'custom_url',\n",
    "                    'publishedAt': 'published_at',\n",
    "                    'country': 'country',\n",
    "                    'viewCount': 'view_count',\n",
    "                    'subscriberCount': 'subscriber_count',\n",
    "                    'videoCount': 'video_count',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  })\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Channel MD.\")\n",
    "\n",
    "\n",
    "logging.info(\"Start: Loading Video MD.\")\n",
    "# Load Video MD\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_video_md',\n",
    "     stage_table_name = 'tbl_stg_yt_video_md',\n",
    "     core_table_name = 'tbl_yt_video_md',\n",
    "     s3_col_map = {\n",
    "                    'id': 'id',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'title': 'title',\n",
    "                    'url': 'url',\n",
    "                    'publishedAt': 'published_at',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  },\n",
    "     load_type = 'MERGE',\n",
    "     merge_on_col = ['id']\n",
    "        )\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Video MD.\")\n",
    "\n",
    "\n",
    "logging.info(\"Start: Loading Video Stats.\")\n",
    "# Load Video stats\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_video_stats',\n",
    "     stage_table_name = 'tbl_stg_yt_video_stats',\n",
    "     core_table_name = 'tbl_yt_video_stats',\n",
    "     s3_col_map = {\n",
    "                    'id': 'id',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'rptg_dt': 'rptg_dt',\n",
    "                    'views': 'view_count',\n",
    "                    'likes': 'like_count',\n",
    "                    'dislikes': 'dislike_count',\n",
    "                    'comments': 'comment_count',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  },\n",
    "     load_type = 'MERGE',\n",
    "     merge_on_col = ['id','rptg_dt']\n",
    ")\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Video Stats.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4695f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
