{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c97d6e",
   "metadata": {},
   "source": [
    "## Define Youtube Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb937f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from typing import Any, Optional, Dict, List\n",
    "import json\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "class CredentialError(Exception):\n",
    "    \"\"\"This exception is raised when there is error in creating credential.\"\"\"\n",
    "\n",
    "class YoutubeDataError(Exception):\n",
    "    \"\"\"This exception is raised when there is error in creating Youtube data object.\"\"\"\n",
    "\n",
    "class InsufficientInputError(Exception):\n",
    "    \"\"\"This exception is raised when insufficint info is provided to create channel object.\"\"\"\n",
    "\n",
    "class ChannelNotFoundError(Exception):\n",
    "    \"\"\"This exception is raised when a channel is not found.\"\"\"\n",
    "\n",
    "class YoutubeChannel():\n",
    "    def __init__(self,\n",
    "                 service_account_info: json,\n",
    "                 scopes: Optional[list] = ['https://www.googleapis.com/auth/youtube.readonly'],\n",
    "                 channel_name: Optional[str] = None,\n",
    "                channel_id: Optional[str] = None) -> None:\n",
    "        # Trying to create the credential object\n",
    "        try:\n",
    "            credentials = service_account.Credentials.from_service_account_info(\n",
    "            service_account_info,\n",
    "            scopes=scopes\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise CredentialError(e)\n",
    "        \n",
    "        # Trying to build the youtube object\n",
    "        try:\n",
    "            youtube = build('youtube', 'v3', credentials=credentials)\n",
    "        except Exception as e:\n",
    "            raise YoutubeDataError(e)\n",
    "            \n",
    "        # Fetching Channel id by name\n",
    "        if channel_name is None and channel_id is None:\n",
    "            raise InsufficientInputError(\"Either channel_name or channel_id needs to be provided\")\n",
    "        \n",
    "        if channel_id is None:\n",
    "            # Fetch channel ID by name\n",
    "            response = youtube.search().list(q=channel_name, type='channel', part='id', maxResults=1).execute()\n",
    "            if not response.get('items'):\n",
    "                raise ChannelNotFoundError(f\"No channel found for username: {channel_name}\")\n",
    "            channel_id = response['items'][0]['id']['channelId']\n",
    "        else:\n",
    "            # Verify the channel id passed is correct\n",
    "            if not youtube.channels().list(id=channel_id,part='id').execute().get('items'):\n",
    "                raise ChannelNotFoundError(f\"No channel found for channel_id: {channel_id}\")\n",
    "        \n",
    "        # Get channel attributes\n",
    "        response = youtube.channels().list(\n",
    "            id=channel_id,\n",
    "            part='snippet,statistics'\n",
    "        ).execute()\n",
    "        \n",
    "        # Set Channel attribute for the object\n",
    "        self._credentials = credentials\n",
    "        self._youtube = youtube\n",
    "        self.channel_name = channel_name\n",
    "        self.channel_id = channel_id\n",
    "        self.title = response.get('items')[0].get('snippet').get('title')\n",
    "        self.description = response.get('items')[0].get('snippet').get('description')\n",
    "        self.customUrl = response.get('items')[0].get('snippet').get('customUrl')\n",
    "        self.publishedAt = response.get('items')[0].get('snippet').get('publishedAt')\n",
    "        self.country = response.get('items')[0].get('snippet').get('country')\n",
    "        self.viewCount = response.get('items')[0].get('statistics').get('viewCount')\n",
    "        self.subscriberCount = response.get('items')[0].get('statistics').get('subscriberCount')\n",
    "        self.videoCount = response.get('items')[0].get('statistics').get('videoCount')\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_video_statistics(youtube: Any, video_ids: list) -> list:\n",
    "        video_stats = []\n",
    "\n",
    "        # Fetch statistics for the videos\n",
    "        video_response = youtube.videos().list(\n",
    "            part='snippet,statistics',\n",
    "            id=','.join(video_ids)\n",
    "        ).execute()\n",
    "\n",
    "        for item in video_response['items']:\n",
    "            video_stats.append({\n",
    "                'id': item['id'],\n",
    "                'title': item['snippet']['title'],\n",
    "                'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                'views': item['statistics'].get('viewCount',0),\n",
    "                'likes': item['statistics'].get('likeCount',0),\n",
    "                'dislikes': item['statistics'].get('dislikeCount',0),\n",
    "                'comments': item['statistics'].get('commentCount', 0),\n",
    "                'publishedAt': item['snippet']['publishedAt']\n",
    "            })\n",
    "    \n",
    "        return video_stats\n",
    "\n",
    "    \n",
    "    def get_video_data(self, chunk_size: Optional[int] = 50, days_count: Optional[int] = 365):\n",
    "        video_data = []\n",
    "        \n",
    "        # Calculating published_after based on days_count\n",
    "        t_ago = datetime.datetime.now() - datetime.timedelta(days=days_count)\n",
    "        published_after = t_ago.isoformat(\"T\") + \"Z\"\n",
    "\n",
    "        request = self._youtube.search().list(\n",
    "                    part='id',\n",
    "                    channelId=self.channel_id,\n",
    "                    publishedAfter=published_after,\n",
    "                    maxResults=chunk_size,\n",
    "                    type='video'\n",
    "                )\n",
    "\n",
    "        while request:\n",
    "            response = request.execute()\n",
    "            video_ids = [item['id']['videoId'] for item in response['items']]\n",
    "            \n",
    "            # Getting video statistics\n",
    "            video_data = video_data + self.get_video_statistics(self._youtube, video_ids)\n",
    "            \n",
    "            # Creating request for the next chunk fetch\n",
    "            request = self._youtube.search().list_next(request, response)        \n",
    "                \n",
    "        return video_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091b81d7",
   "metadata": {},
   "source": [
    "## Define Snowflake Loader class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631820fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "from typing import Any, Optional, Dict, List\n",
    "import logging\n",
    "        \n",
    "        \n",
    "class SnowflakeLoader():\n",
    "    def __init__(self,\n",
    "                 conn: Any,\n",
    "                 schema: str,\n",
    "                 s3_stage_name: str,\n",
    "                 stage_table_name: str,\n",
    "                 core_table_name: str,\n",
    "                 s3_col_map: Dict,\n",
    "                 load_type: Optional[str] = 'FULL',\n",
    "                 merge_on_col: Optional[List[str]] = []\n",
    "                ):\n",
    "        try:\n",
    "            curs = conn.cursor()\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error while opening the cursor. {e}\")\n",
    "        \n",
    "        # Check when load type is Merge, the merge_on_col need to be supplied\n",
    "        if load_type.upper() == 'MERGE' and len(merge_on_col) == 0:\n",
    "            raise Exception(f\"merge_on_col arg is mandatory for Loader type: {load_type}\")\n",
    "        \n",
    "        # Fecthing stage table columns\n",
    "        curs.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{stage_table_name.upper()}' AND TABLE_SCHEMA = '{schema.upper()}';\n",
    "        \"\"\")\n",
    "        self.stg_cols = [row[0] for row in curs.fetchall()]\n",
    "        \n",
    "        # Fecthing main table columns\n",
    "        curs.execute(f\"\"\"\n",
    "        SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{core_table_name.upper()}' AND TABLE_SCHEMA = '{schema.upper()}';\n",
    "        \"\"\")\n",
    "        self.main_cols = [row[0] for row in curs.fetchall()]\n",
    "        \n",
    "        # Checking if stage and main table columns are in sync\n",
    "        if not set(self.main_cols) == set(self.stg_cols):\n",
    "            raise Exception('Stage and Main table column mismatch. ')\n",
    "        \n",
    "        # Converting merge columns to upper\n",
    "        if  len(merge_on_col) != 0:\n",
    "            self.merge_on_col = [col.upper() for col in merge_on_col]\n",
    "        else:\n",
    "            self.merge_on_col = merge_on_col\n",
    "            \n",
    "        # Checking if merge col existing in stage and main table\n",
    "        if load_type.upper() == 'MERGE':\n",
    "            if len([col for col in self.merge_on_col if col in self.main_cols and col in self.stg_cols]) != len(self.merge_on_col):\n",
    "                raise Exception('All columns in merge_on_col must be present in both stage and core table.')\n",
    "        \n",
    "            \n",
    "        self.schema = schema.upper()\n",
    "        self.stage_table_name = stage_table_name.upper()\n",
    "        self.core_table_name = core_table_name.upper()\n",
    "        self.load_type = load_type.upper()\n",
    "        self.conn = conn\n",
    "        self.s3_col_map = s3_col_map\n",
    "        self.s3_stage_name = s3_stage_name\n",
    "        \n",
    "        curs.close()\n",
    "    \n",
    "    def curs_handler(func):\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "            try:\n",
    "                curs = self.conn.cursor()\n",
    "            except Exception as e:\n",
    "                raise Exception(f\"from db_conn_check: Error while opening the cursor. {e}\")\n",
    "            func(self, curs, *args, **kwargs)\n",
    "            curs.close()\n",
    "        return wrapper\n",
    "\n",
    "    \n",
    "    @curs_handler\n",
    "    def stg_to_core(self, curs, *args, **kwargs) -> None:\n",
    "            \n",
    "        if self.load_type == 'MERGE':\n",
    "            sql_text = [f\"\"\"\n",
    "                MERGE INTO {self.schema}.{self.core_table_name} as t\n",
    "                USING {self.schema}.{self.stage_table_name} as d\n",
    "                ON \n",
    "                {'AND '.join(f\"d.{col} = t.{col} \" for col in self.merge_on_col)}\n",
    "                WHEN MATCHED THEN\n",
    "                UPDATE SET\n",
    "                {', '.join(f\"{col} = d.{col} \" for col in [col for col in self.main_cols if col not in self.merge_on_col])}\n",
    "                WHEN NOT MATCHED THEN\n",
    "                INSERT\n",
    "                ({','.join([col for col in self.main_cols])})\n",
    "                VALUES\n",
    "                ({','.join([f\"d.{col}\" for col in self.main_cols])})\n",
    "                ;\n",
    "                \"\"\"]\n",
    "        else:\n",
    "            sql_text = [f\"delete from {self.schema}.{self.core_table_name};\",\n",
    "            f\"\"\"insert into {self.schema}.{self.core_table_name}\n",
    "            ({','.join([col for col in self.main_cols])})\n",
    "            select\n",
    "            {','.join([col for col in self.main_cols])}\n",
    "            from\n",
    "            {self.schema}.{self.stage_table_name};\n",
    "            \"\"\"]\n",
    "        \n",
    "        for sql in sql_text:\n",
    "            curs.execute(sql)\n",
    "            logging.info(sql)\n",
    "\n",
    "    @curs_handler  \n",
    "    def s3_to_stg(self, curs, *args, **kwargs) -> None:\n",
    "        \n",
    "        # Validate S3_col_map dictionary if available\n",
    "        for item in self.s3_col_map.items():\n",
    "            if item[1].upper() not in self.stg_cols:\n",
    "                raise Exception(f\"s3_to_stg: Column {item[1]} not present in the table {self.schema}.{self.stage_table_name}\")\n",
    "                    \n",
    "        \n",
    "        # Cleanup stage table first\n",
    "        curs.execute(f\"\"\"delete from {self.schema}.{self.stage_table_name};\"\"\")\n",
    "        \n",
    "        # Prepare the COPY INTO statement for S3 load\n",
    "        sql_text = f\"\"\"\n",
    "        COPY INTO {self.schema}.{self.stage_table_name} ({','.join([col for col in [item[1] for item in self.s3_col_map.items()]])})\n",
    "        FROM (\n",
    "            SELECT\n",
    "            {','.join([f\"$1:{item[0]}::VARIANT AS {item[1]}\" for item in self.s3_col_map.items()])}\n",
    "            FROM @{self.s3_stage_name}\n",
    "        )\n",
    "        FILE_FORMAT = (TYPE = 'PARQUET');\n",
    "        \"\"\"\n",
    "        \n",
    "        logging.info(sql_text)\n",
    "        curs.execute(sql_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede6364",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4352c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_info = json.load(open('Secrets/youtube-analytics-sph-2-1a7dea22ffc9.json'))\n",
    "channel_list = ['straitstimesonline', 'BeritaHarianSG1957', 'Tamil_Murasu', 'TheBusinessTimes', 'zaobaodotsg']\n",
    "temp_directory = './temp_download'\n",
    "s3_bucket_name = 'youtube-stats-001'\n",
    "s3_path = \"dump/parquet\"\n",
    "aws_access_key_id = \"***********\"\n",
    "aws_secret_access_key = \"************\"\n",
    "sf_username = '***********'\n",
    "sf_password = '***********'\n",
    "sf_account = '**********'\n",
    "sf_warehouse = 'COMPUTE_WH'\n",
    "sf_database = 'TESTDB'\n",
    "sf_schema = 'CORE'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac48412",
   "metadata": {},
   "source": [
    "## Fetch from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe388676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "df_channel = pd.DataFrame(columns=['channel_name','channel_id','title','customUrl','publishedAt','country','viewCount','subscriberCount','videoCount','rptg_dt','etl_ts'])\n",
    "df_video = pd.DataFrame(columns=['id','title','url','views','likes','dislikes','comments','publishedAt','channel_name','channel_id','rptg_dt','etl_ts'])\n",
    "\n",
    "_now_ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "_today_dt = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "logging.info(f\"_now_ts: {_now_ts}\")\n",
    "logging.info(f\"_today_dt: {_today_dt}\")\n",
    "\n",
    "for channel_name in channel_list:\n",
    "    logging.info(f\"Fetching data for Channel: {channel_name}\")\n",
    "    channelObj = YoutubeChannel(service_account_info=service_account_info, channel_name=channel_name)    \n",
    "    df_channel = pd.concat([df_channel, pd.DataFrame([{\n",
    "        'channel_name': channelObj.channel_name,\n",
    "        'channel_id': channelObj.channel_id,\n",
    "        'title': channelObj.title,\n",
    "        'customUrl': channelObj.customUrl,\n",
    "        'publishedAt': channelObj.publishedAt,\n",
    "        'country': channelObj.country,\n",
    "        'viewCount': channelObj.viewCount,\n",
    "        'subscriberCount': channelObj.subscriberCount,\n",
    "        'videoCount': channelObj.videoCount,\n",
    "        'rptg_dt': _today_dt,\n",
    "        'etl_ts': _now_ts\n",
    "    }])], ignore_index=True)\n",
    "\n",
    "    \n",
    "    df_video_temp = pd.DataFrame(channelObj.get_video_data())\n",
    "    df_video_temp['channel_name'] = channelObj.channel_name\n",
    "    df_video_temp['channel_id'] = channelObj.channel_id\n",
    "    df_video_temp['rptg_dt'] = _today_dt\n",
    "    df_video_temp['etl_ts'] = _now_ts\n",
    "    df_video = pd.concat([df_video, df_video_temp], ignore_index=True)\n",
    "\n",
    "df_video_md = df_video[['id','channel_id','title','url','publishedAt','etl_ts']]\n",
    "df_video = df_video[['id','channel_id','rptg_dt','views','likes','dislikes','comments','etl_ts']]\n",
    "df_channel_md = df_channel[['channel_name','channel_id','title','customUrl','publishedAt','country','etl_ts']]\n",
    "df_channel = df_channel[['channel_id','rptg_dt','viewCount','subscriberCount','videoCount','etl_ts']]\n",
    "\n",
    "# Dropping duplicates based on respective Key columns\n",
    "df_channel = df_channel.drop_duplicates(subset=['channel_id','rptg_dt'])\n",
    "df_channel_md = df_channel_md.drop_duplicates(subset=['channel_id'])\n",
    "df_video_md = df_video_md.drop_duplicates(subset=['id'])\n",
    "df_video = df_video.drop_duplicates(subset=['id','rptg_dt'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e9489a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_name</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>customUrl</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>country</th>\n",
       "      <th>etl_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>straitstimesonline</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>The Straits Times</td>\n",
       "      <td>@straitstimesonline</td>\n",
       "      <td>2011-09-30T11:06:35Z</td>\n",
       "      <td>SG</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BeritaHarianSG1957</td>\n",
       "      <td>UC_WgSFSkn7112rmJQcHSUIQ</td>\n",
       "      <td>Berita Harian Singapura</td>\n",
       "      <td>@beritahariansg1957</td>\n",
       "      <td>2013-12-10T10:54:31Z</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tamil_Murasu</td>\n",
       "      <td>UCs0xZ60FSNxFxHPVFFsXNTA</td>\n",
       "      <td>Tamil Murasu</td>\n",
       "      <td>@tamil_murasu</td>\n",
       "      <td>2016-04-20T09:01:10Z</td>\n",
       "      <td>SG</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheBusinessTimes</td>\n",
       "      <td>UC0GP1HDhGZTLih7B89z_cTg</td>\n",
       "      <td>The Business Times</td>\n",
       "      <td>@thebusinesstimes</td>\n",
       "      <td>2013-11-13T11:10:40Z</td>\n",
       "      <td>SG</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zaobaodotsg</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>zaobaosg</td>\n",
       "      <td>@zaobaodotsg</td>\n",
       "      <td>2013-12-04T06:35:33Z</td>\n",
       "      <td>SG</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         channel_name                channel_id                    title  \\\n",
       "0  straitstimesonline  UC4p_I9eiRewn2KoU-nawrDg        The Straits Times   \n",
       "1  BeritaHarianSG1957  UC_WgSFSkn7112rmJQcHSUIQ  Berita Harian Singapura   \n",
       "2        Tamil_Murasu  UCs0xZ60FSNxFxHPVFFsXNTA             Tamil Murasu   \n",
       "3    TheBusinessTimes  UC0GP1HDhGZTLih7B89z_cTg       The Business Times   \n",
       "4         zaobaodotsg  UCrbQxu0YkoVWu2dw5b1MzNg                 zaobaosg   \n",
       "\n",
       "             customUrl           publishedAt country               etl_ts  \n",
       "0  @straitstimesonline  2011-09-30T11:06:35Z      SG  2024-08-09 23:15:57  \n",
       "1  @beritahariansg1957  2013-12-10T10:54:31Z    None  2024-08-09 23:15:57  \n",
       "2        @tamil_murasu  2016-04-20T09:01:10Z      SG  2024-08-09 23:15:57  \n",
       "3    @thebusinesstimes  2013-11-13T11:10:40Z      SG  2024-08-09 23:15:57  \n",
       "4         @zaobaodotsg  2013-12-04T06:35:33Z      SG  2024-08-09 23:15:57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_id</th>\n",
       "      <th>rptg_dt</th>\n",
       "      <th>viewCount</th>\n",
       "      <th>subscriberCount</th>\n",
       "      <th>videoCount</th>\n",
       "      <th>etl_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>373492135</td>\n",
       "      <td>589000</td>\n",
       "      <td>30445</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UC_WgSFSkn7112rmJQcHSUIQ</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>1398914</td>\n",
       "      <td>5010</td>\n",
       "      <td>451</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UCs0xZ60FSNxFxHPVFFsXNTA</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>1341355</td>\n",
       "      <td>4850</td>\n",
       "      <td>457</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC0GP1HDhGZTLih7B89z_cTg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>4241994</td>\n",
       "      <td>27100</td>\n",
       "      <td>1270</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>45422764</td>\n",
       "      <td>182000</td>\n",
       "      <td>5327</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 channel_id     rptg_dt  viewCount subscriberCount videoCount  \\\n",
       "0  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09  373492135          589000      30445   \n",
       "1  UC_WgSFSkn7112rmJQcHSUIQ  2024-08-09    1398914            5010        451   \n",
       "2  UCs0xZ60FSNxFxHPVFFsXNTA  2024-08-09    1341355            4850        457   \n",
       "3  UC0GP1HDhGZTLih7B89z_cTg  2024-08-09    4241994           27100       1270   \n",
       "4  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09   45422764          182000       5327   \n",
       "\n",
       "                etl_ts  \n",
       "0  2024-08-09 23:15:57  \n",
       "1  2024-08-09 23:15:57  \n",
       "2  2024-08-09 23:15:57  \n",
       "3  2024-08-09 23:15:57  \n",
       "4  2024-08-09 23:15:57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>etl_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qicZBzvx32U</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>US President Biden stumbles over his words dur...</td>\n",
       "      <td>https://www.youtube.com/watch?v=qicZBzvx32U</td>\n",
       "      <td>2024-06-28T04:53:14Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZX-L_On-1U</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>How to score better in #floorball</td>\n",
       "      <td>https://www.youtube.com/watch?v=AZX-L_On-1U</td>\n",
       "      <td>2024-02-01T09:03:18Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5woRDRWdol8</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>Why Biden is passing the torch #uspresidential...</td>\n",
       "      <td>https://www.youtube.com/watch?v=5woRDRWdol8</td>\n",
       "      <td>2024-07-25T10:15:48Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_GEiW4JDdJU</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>Long queues at Changi Airport as major tech ou...</td>\n",
       "      <td>https://www.youtube.com/watch?v=_GEiW4JDdJU</td>\n",
       "      <td>2024-07-19T08:38:35Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jgA7eJ4BQJI</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>China football fans love Singapore goalkeeper ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=jgA7eJ4BQJI</td>\n",
       "      <td>2024-06-13T11:47:56Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>z_HJpLhxnwE</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>【新闻抢先看】坠假官员骗局 19岁女称：误助骗子诈四长者</td>\n",
       "      <td>https://www.youtube.com/watch?v=z_HJpLhxnwE</td>\n",
       "      <td>2024-02-05T10:12:38Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>3QBMoCMOi-U</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>【新闻抢先看】码头人挤人 国人峇淡岛返新苦等四小时登船</td>\n",
       "      <td>https://www.youtube.com/watch?v=3QBMoCMOi-U</td>\n",
       "      <td>2024-02-13T21:12:55Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>dseu8pMSCJU</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>【新闻抢先看】28岁丧夫独养六孩 单肾嬷坚守鱼摊50年</td>\n",
       "      <td>https://www.youtube.com/watch?v=dseu8pMSCJU</td>\n",
       "      <td>2024-01-30T10:35:15Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>7YclzmRQ3Ak</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>封口费案封不了特朗普竞选美国总统之路 Trump’s guilty verdict #东谈西...</td>\n",
       "      <td>https://www.youtube.com/watch?v=7YclzmRQ3Ak</td>\n",
       "      <td>2024-06-04T09:24:10Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>k34bbshc6zQ</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>中菲“录音门”会如何收场？China-Philippines tensions heat u...</td>\n",
       "      <td>https://www.youtube.com/watch?v=k34bbshc6zQ</td>\n",
       "      <td>2024-05-14T14:19:48Z</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                channel_id  \\\n",
       "0     qicZBzvx32U  UC4p_I9eiRewn2KoU-nawrDg   \n",
       "1     AZX-L_On-1U  UC4p_I9eiRewn2KoU-nawrDg   \n",
       "2     5woRDRWdol8  UC4p_I9eiRewn2KoU-nawrDg   \n",
       "3     _GEiW4JDdJU  UC4p_I9eiRewn2KoU-nawrDg   \n",
       "4     jgA7eJ4BQJI  UC4p_I9eiRewn2KoU-nawrDg   \n",
       "...           ...                       ...   \n",
       "2007  z_HJpLhxnwE  UCrbQxu0YkoVWu2dw5b1MzNg   \n",
       "2008  3QBMoCMOi-U  UCrbQxu0YkoVWu2dw5b1MzNg   \n",
       "2009  dseu8pMSCJU  UCrbQxu0YkoVWu2dw5b1MzNg   \n",
       "2010  7YclzmRQ3Ak  UCrbQxu0YkoVWu2dw5b1MzNg   \n",
       "2011  k34bbshc6zQ  UCrbQxu0YkoVWu2dw5b1MzNg   \n",
       "\n",
       "                                                  title  \\\n",
       "0     US President Biden stumbles over his words dur...   \n",
       "1                     How to score better in #floorball   \n",
       "2     Why Biden is passing the torch #uspresidential...   \n",
       "3     Long queues at Changi Airport as major tech ou...   \n",
       "4     China football fans love Singapore goalkeeper ...   \n",
       "...                                                 ...   \n",
       "2007                       【新闻抢先看】坠假官员骗局 19岁女称：误助骗子诈四长者   \n",
       "2008                        【新闻抢先看】码头人挤人 国人峇淡岛返新苦等四小时登船   \n",
       "2009                        【新闻抢先看】28岁丧夫独养六孩 单肾嬷坚守鱼摊50年   \n",
       "2010  封口费案封不了特朗普竞选美国总统之路 Trump’s guilty verdict #东谈西...   \n",
       "2011  中菲“录音门”会如何收场？China-Philippines tensions heat u...   \n",
       "\n",
       "                                              url           publishedAt  \\\n",
       "0     https://www.youtube.com/watch?v=qicZBzvx32U  2024-06-28T04:53:14Z   \n",
       "1     https://www.youtube.com/watch?v=AZX-L_On-1U  2024-02-01T09:03:18Z   \n",
       "2     https://www.youtube.com/watch?v=5woRDRWdol8  2024-07-25T10:15:48Z   \n",
       "3     https://www.youtube.com/watch?v=_GEiW4JDdJU  2024-07-19T08:38:35Z   \n",
       "4     https://www.youtube.com/watch?v=jgA7eJ4BQJI  2024-06-13T11:47:56Z   \n",
       "...                                           ...                   ...   \n",
       "2007  https://www.youtube.com/watch?v=z_HJpLhxnwE  2024-02-05T10:12:38Z   \n",
       "2008  https://www.youtube.com/watch?v=3QBMoCMOi-U  2024-02-13T21:12:55Z   \n",
       "2009  https://www.youtube.com/watch?v=dseu8pMSCJU  2024-01-30T10:35:15Z   \n",
       "2010  https://www.youtube.com/watch?v=7YclzmRQ3Ak  2024-06-04T09:24:10Z   \n",
       "2011  https://www.youtube.com/watch?v=k34bbshc6zQ  2024-05-14T14:19:48Z   \n",
       "\n",
       "                   etl_ts  \n",
       "0     2024-08-09 23:15:57  \n",
       "1     2024-08-09 23:15:57  \n",
       "2     2024-08-09 23:15:57  \n",
       "3     2024-08-09 23:15:57  \n",
       "4     2024-08-09 23:15:57  \n",
       "...                   ...  \n",
       "2007  2024-08-09 23:15:57  \n",
       "2008  2024-08-09 23:15:57  \n",
       "2009  2024-08-09 23:15:57  \n",
       "2010  2024-08-09 23:15:57  \n",
       "2011  2024-08-09 23:15:57  \n",
       "\n",
       "[1891 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>channel_id</th>\n",
       "      <th>rptg_dt</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comments</th>\n",
       "      <th>etl_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qicZBzvx32U</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>20123</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZX-L_On-1U</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>17998</td>\n",
       "      <td>413</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5woRDRWdol8</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>2695</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_GEiW4JDdJU</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>16209</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jgA7eJ4BQJI</td>\n",
       "      <td>UC4p_I9eiRewn2KoU-nawrDg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>21979</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>z_HJpLhxnwE</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>2035</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>3QBMoCMOi-U</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>2840</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009</th>\n",
       "      <td>dseu8pMSCJU</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>2449</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>7YclzmRQ3Ak</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>3245</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>k34bbshc6zQ</td>\n",
       "      <td>UCrbQxu0YkoVWu2dw5b1MzNg</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>14224</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>2024-08-09 23:15:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                channel_id     rptg_dt  views likes dislikes  \\\n",
       "0     qicZBzvx32U  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09  20123   163        0   \n",
       "1     AZX-L_On-1U  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09  17998   413        0   \n",
       "2     5woRDRWdol8  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09   2695    41        0   \n",
       "3     _GEiW4JDdJU  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09  16209   113        0   \n",
       "4     jgA7eJ4BQJI  UC4p_I9eiRewn2KoU-nawrDg  2024-08-09  21979   250        0   \n",
       "...           ...                       ...         ...    ...   ...      ...   \n",
       "2007  z_HJpLhxnwE  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09   2035    22        0   \n",
       "2008  3QBMoCMOi-U  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09   2840    41        0   \n",
       "2009  dseu8pMSCJU  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09   2449    32        0   \n",
       "2010  7YclzmRQ3Ak  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09   3245    50        0   \n",
       "2011  k34bbshc6zQ  UCrbQxu0YkoVWu2dw5b1MzNg  2024-08-09  14224    91        0   \n",
       "\n",
       "     comments               etl_ts  \n",
       "0          42  2024-08-09 23:15:57  \n",
       "1           2  2024-08-09 23:15:57  \n",
       "2          12  2024-08-09 23:15:57  \n",
       "3          14  2024-08-09 23:15:57  \n",
       "4          12  2024-08-09 23:15:57  \n",
       "...       ...                  ...  \n",
       "2007        1  2024-08-09 23:15:57  \n",
       "2008        1  2024-08-09 23:15:57  \n",
       "2009        0  2024-08-09 23:15:57  \n",
       "2010       12  2024-08-09 23:15:57  \n",
       "2011       75  2024-08-09 23:15:57  \n",
       "\n",
       "[1891 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_channel_md)\n",
    "display(df_channel)\n",
    "display(df_video_md)\n",
    "display(df_video)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f8715",
   "metadata": {},
   "source": [
    "## Write to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe581927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import logging\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.resource('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "# Upload the Channel MD Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_channel_md.to_parquet(parquet_buffer, index=False)\n",
    "    chnl_md_file_name = f\"channel_md_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/channel_md/{chnl_md_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {chnl_md_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/channel_md\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Channel MD S3 Upload failed. {e}\")\n",
    "\n",
    "    \n",
    "# Upload the Channel Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_channel.to_parquet(parquet_buffer, index=False)\n",
    "    chnl_file_name = f\"channel_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/channel/{chnl_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {chnl_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/channel\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Channel Stats S3 Upload failed. {e}\")\n",
    "\n",
    "# Upload the Video MD Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_video_md.to_parquet(parquet_buffer, index=False)\n",
    "    video_md_file_name = f\"video_md_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/video_md/{video_md_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {video_md_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/video_md\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Video MD S3 Upload failed. {e}\")\n",
    "\n",
    "# Upload the Video Parquet file to S3\n",
    "try:\n",
    "    parquet_buffer = io.BytesIO()\n",
    "    df_video.to_parquet(parquet_buffer, index=False)\n",
    "    video_file_name = f\"video_data_{str(int(round(time.time())))}.parquet\"\n",
    "    s3.Object(s3_bucket_name, f\"{s3_path}/video/{video_file_name}\").put(Body=parquet_buffer.getvalue())\n",
    "    logging.info(f\"File {video_file_name} has been uploaded to s3://{s3_bucket_name}/{s3_path}/video\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Video Stats S3 Upload failed. {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c52124",
   "metadata": {},
   "source": [
    "## Load to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b6e4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/cryptography/hazmat/backends/openssl/x509.py:14: CryptographyDeprecationWarning: This version of cryptography contains a temporary pyOpenSSL fallback path. Upgrade pyOpenSSL now.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "dbcon = snowflake.connector.connect(\n",
    "    user=sf_username,\n",
    "    password=sf_password,\n",
    "    account=sf_account,\n",
    "    warehouse=sf_warehouse,\n",
    "    database=sf_database,\n",
    "    schema=sf_schema\n",
    ")\n",
    "\n",
    "logging.info(\"Start: Loading Channel MD.\")\n",
    "# Load Channel MD\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_channel_md',\n",
    "     stage_table_name = 'tbl_stg_yt_channel_md',\n",
    "     core_table_name = 'tbl_yt_channel_md',\n",
    "     s3_col_map = {\n",
    "                    'channel_name': 'channel_name',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'title': 'title',\n",
    "                    'customUrl': 'custom_url',\n",
    "                    'publishedAt': 'published_at',\n",
    "                    'country': 'country',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  })\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Channel MD.\")\n",
    "\n",
    "\n",
    "logging.info(\"Start: Loading Channel Stats.\")\n",
    "# Load Channel Stats\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_channel_stats',\n",
    "     stage_table_name = 'tbl_stg_yt_channel_stats',\n",
    "     core_table_name = 'tbl_yt_channel_stats',\n",
    "     s3_col_map = {\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'rptg_dt': 'rptg_dt',\n",
    "                    'viewCount': 'view_count',\n",
    "                    'subscriberCount': 'subscriber_count',\n",
    "                    'videoCount': 'video_count',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  },\n",
    "     load_type = 'MERGE',\n",
    "     merge_on_col = ['channel_id','rptg_dt']\n",
    "        )\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Channel Stats.\")\n",
    "\n",
    "\n",
    "logging.info(\"Start: Loading Video MD.\")\n",
    "# Load Video MD\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_video_md',\n",
    "     stage_table_name = 'tbl_stg_yt_video_md',\n",
    "     core_table_name = 'tbl_yt_video_md',\n",
    "     s3_col_map = {\n",
    "                    'id': 'id',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'title': 'title',\n",
    "                    'url': 'url',\n",
    "                    'publishedAt': 'published_at',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  },\n",
    "     load_type = 'MERGE',\n",
    "     merge_on_col = ['id']\n",
    "        )\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Video MD.\")\n",
    "\n",
    "\n",
    "logging.info(\"Start: Loading Video Stats.\")\n",
    "# Load Video stats\n",
    "sf_ldr = SnowflakeLoader(\n",
    "     conn = dbcon,\n",
    "     schema = sf_schema,\n",
    "     s3_stage_name = 'stg_yt_video_stats',\n",
    "     stage_table_name = 'tbl_stg_yt_video_stats',\n",
    "     core_table_name = 'tbl_yt_video_stats',\n",
    "     s3_col_map = {\n",
    "                    'id': 'id',\n",
    "                    'channel_id': 'channel_id',\n",
    "                    'rptg_dt': 'rptg_dt',\n",
    "                    'views': 'view_count',\n",
    "                    'likes': 'like_count',\n",
    "                    'dislikes': 'dislike_count',\n",
    "                    'comments': 'comment_count',\n",
    "                    'etl_ts': 'etl_ts'\n",
    "                  },\n",
    "     load_type = 'MERGE',\n",
    "     merge_on_col = ['id','rptg_dt']\n",
    ")\n",
    "\n",
    "# Run the S3 loader and core loader\n",
    "sf_ldr.s3_to_stg()\n",
    "sf_ldr.stg_to_core()\n",
    "logging.info(\"Complete: Loading Video Stats.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dad404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
